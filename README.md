# Liorhybrid

A PyTorch implementation of physics-inspired AI combining Bayesian cognitive field dynamics with geometric algebra for interpretable, efficient learning.

## Overview

Liorhybrid is a research framework that bridges theoretical physics and modern machine learning. It implements a **Cognitive Tensor Field** $T_{ij}(x,t)$ that evolves under physics-inspired dynamics, integrated with transformer architectures via geometric attention mechanisms.

### Core Components

**1. Cognitive Tensor Field**
- Rank-2 complex tensor field $T_{ij}(x,t) \in \mathbb{C}^{D \times D}$ at each spatial location
- Evolves via Bayesian recursive dynamics with fractional memory
- Governed by master equation: $i\hbar_{cog} \partial_t T = H[T] + \Lambda_{QR}[T] - \Lambda_F[T] + J$
- Hamiltonian evolution + Bayesian updates + power-law memory kernel

**2. LIoR (Learning in Operator Regime)**
- Geodesic-based optimization through field Hamiltonians
- Parameters update via entropy gradients $\nabla H$ rather than loss gradients $\nabla L$
- Geodesic cost measures deviation from physics-guided optimal paths
- O(1) recurrence for efficient memory kernel computation

**3. Geometric Attention**
- Replaces standard dot-product attention with geometric products
- **Wedge product**: Antisymmetric (captures orthogonality between concepts)
- **Tensor product**: Symmetric (captures correlations)
- **Spinor product**: Rotational invariants (captures phase structure)
- Field-contracted operations avoid OOM on large tensors

**4. Biquaternion Algebra**
- 16-DOF state space: two complex quaternions (Q_M for present, Q_H for memory)
- Pure real arithmetic (fp16/bf16 compatible, avoids ComplexHalf bugs)
- SL(2,C) transformations represent Lorentz rotations + boosts in cognitive spacetime

### Key Features

- **Physics-Guided Learning**: Evolution driven by physical principles, not just gradient descent
- **Interpretable Representations**: Field dynamics have clear mathematical/physical meaning
- **Memory Efficient**: Field contractions reduce O(dÂ²) outer products to O(d) operations
- **Adaptive Parameters**: Field parameters (Î±, Î½, Ï„) learn optimal values during training
- **Multi-modal**: Supports text, images, and video through field encoding
- **GPU Accelerated**: Full PyTorch with CUDA support

## Installation

### From source

```bash
git clone https://github.com/JewJitsu11B/Liorhybrid.git
cd Liorhybrid
pip install -e .
```

### Dependencies

**Required:**
- Python â‰¥ 3.8
- PyTorch â‰¥ 2.0.0
- NumPy â‰¥ 1.21.0
- SciPy â‰¥ 1.7.0

**Optional (for DPR K/V generation):**
- transformers (HuggingFace)

**Development:**
- pytest â‰¥ 7.0.0
- matplotlib â‰¥ 3.5.0 (for visualization)

Install all dependencies:
```bash
pip install -r requirements.txt
```

## Quick Start

### Interactive Training

Launch the interactive training interface:

```bash
python main.py
```

Available options:
1. Quick Start (Geometric Training - Recommended)
2. Full Training (Train Everything End-to-End)
3. Resume from Checkpoint
4. Generate Sample Dataset
5. Inference/Chat Mode
6. Inspect Checkpoint
7. Evaluate Checkpoint (Run Validation)
8. Config Cost Calculator
9. Exit

### Basic Field Evolution

```python
from Liorhybrid import CognitiveTensorField, FAST_TEST_CONFIG

# Create field with default configuration
field = CognitiveTensorField(FAST_TEST_CONFIG)

# Run evolution
for step in range(100):
    field.evolve_step()
    
    if step % 20 == 0:
        print(f"Step {step}: ||T||Â² = {field.get_norm_squared():.6f}")
```

### Transformer Training with LIoR

```python
from Liorhybrid.core import CognitiveTensorField, FieldConfig
from Liorhybrid.inference import GeometricTransformer
from Liorhybrid.training import CognitiveTrainer, TextDataset, CognitiveTokenizer

# Initialize field
field_config = FieldConfig(
    spatial_size=(16, 16),
    tensor_dim=16,
    adaptive_learning=True  # Enable adaptive Î±, Î½, Ï„
)
field = CognitiveTensorField(field_config)

# Initialize tokenizer
tokenizer = CognitiveTokenizer()

# Create geometric transformer
model = GeometricTransformer(
    d_model=512,
    n_layers=6,
    n_heads=8,
    field_dim=16,
    field=field  # Connect to cognitive field
)

# Load dataset
dataset = TextDataset("path/to/data.txt", tokenizer, max_length=512)

# Train with LIoR
trainer = CognitiveTrainer(
    model=model,
    field=field,
    tokenizer=tokenizer,
    use_lior=True,           # Enable geodesic optimization
    lior_loss_weights={
        'lm': 1.0,            # Language modeling
        'geodesic': 0.1,      # Geodesic cost
        'field_entropy': 0.001  # Field regularization
    },
    max_epochs=10
)

trainer.train(dataset)
```

## Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cognitive Tensor Field                    â”‚
â”‚   T_ij(x,t) evolves via Bayesian recursive dynamics         â”‚
â”‚   â€¢ Hamiltonian evolution (kinetic + potential)             â”‚
â”‚   â€¢ Bayesian updates (belief revision)                      â”‚
â”‚   â€¢ Fractional memory (power-law kernel)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Provides metric & key/value states
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Geometric Transformer Layer                    â”‚
â”‚   â€¢ Input â†’ Embeddings                                      â”‚
â”‚   â€¢ Query generation from input                             â”‚
â”‚   â€¢ Key/Value extraction from field                         â”‚
â”‚   â€¢ Geometric attention (wedge/tensor/spinor products)      â”‚
â”‚   â€¢ Output generation                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LIoR Training Loop                        â”‚
â”‚   Loss = CrossEntropy + w_geo * GeodesicCost               â”‚
â”‚   â€¢ Standard gradients â†’ model parameters                   â”‚
â”‚   â€¢ Entropy gradients â†’ field parameters (Î±, Î½, Ï„)         â”‚
â”‚   â€¢ Geodesic cost guides optimization through field         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Field Evolution Equation

The cognitive tensor field evolves according to:

```
iâ„_cog âˆ‚_t T_ij = H[T]_ij + Î›_QR[T]_ij - Î›_F[T]_ij + J_ij
```

Where:
- **H[T]**: Hamiltonian operator (kinetic + potential energy)
  ```
  H[T]_ij = -(â„Â²_cog/2m_cog) âˆ‡Â²T_ij + V_ij T_ij
  ```

- **Î›_QR[T]**: Bayesian recursive update (drives toward evidence)
  ```
  Î›_QR[T]_ij = Î»_QR (B[T(t-Î”t)]_ij - T_ij(t-Î”t))
  B[T]_ij = (w_ij T_ij) / Z    where w_ij = exp(-|T_ij - E_ij|Â²/Ï„)
  ```

- **Î›_F[T]**: Fractional memory kernel (long-range temporal correlations)
  ```
  Î›_F[T]_ij = Î»_F âˆ«â‚€áµ— K(t-Ï„) T_ij(Ï„) dÏ„
  K(Ï„) = Ï„^(Î±-1) / Î“(Î±)
  ```

- **J**: External input/stimulus

### LIoR Memory Kernel

Efficient O(1) recurrence for non-Markovian dynamics:

```
K_L(dt) = Î±Â·exp(-Î²Â·dt)                      # Exponential (Markov)
        - Î³Â·dt^(-Î´)Â·exp(-Î¾Â·dt)               # Power-law (Fractional)
        + Î·Â·cos(Ï‰Â·dt + Ï†)Â·exp(-Î¶Â·dt)         # Oscillatory (Phase)
```

State update: `m_t = ÏÂ·m_{t-1} + Î·Â·x_t - Î¾Â·x_{t-p}`

### Geometric Products

Field-contracted attention products (memory-efficient):

**Wedge Product** (antisymmetric):
```python
score(i,j) = Î£_Î¼Î½ T_Î¼Î½ (Q_i^Î¼ K_j^Î½ - K_j^Î¼ Q_i^Î½)
```
- High score = orthogonal concepts (QâŠ¥K)
- Captures novelty and complementarity

**Tensor Product** (symmetric):
```python
score(i,j) = ||Q_i|| Ã— ||K_j|| Ã— Tr(T)
```
- Captures signal strength and correlations
- Modulated by field magnitude

**Spinor Product** (rotational):
```python
score(i,j) = Re(Q_i^â€  Ïƒ_Î¼ K_j) T^Î¼
```
- Extracts rotational invariants
- Captures phase structure and orientation

### Directory Structure

```
Liorhybrid/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py              # Field configuration parameters
â”‚   â””â”€â”€ tensor_field.py        # CognitiveTensorField implementation
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ biquaternion.py        # Biquaternion algebra (16-DOF state)
â”‚   â”œâ”€â”€ lior_kernel.py         # LIoR memory kernel (O(1) recurrence)
â”‚   â”œâ”€â”€ causal_field.py        # Causal field dynamics
â”‚   â”œâ”€â”€ complex_metric.py      # Metric tensor computations
â”‚   â””â”€â”€ manifold.py            # Geometric manifold operations
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ geometric_attention.py # Geometric attention mechanisms
â”‚   â”œâ”€â”€ geometric_products.py  # Wedge/tensor/spinor products
â”‚   â”œâ”€â”€ field_extraction.py    # Extract K/V from field
â”‚   â””â”€â”€ dpr_encoder.py         # DPR K/V generation (optional)
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ trainer.py             # Standard training loop
â”‚   â”œâ”€â”€ lior_trainer.py        # LIoR geodesic training
â”‚   â”œâ”€â”€ lior_optimizer.py      # Entropy-based optimization
â”‚   â”œâ”€â”€ biquat_optimizer.py    # Biquaternion-specific optimizer
â”‚   â”œâ”€â”€ losses.py              # Loss functions (geodesic, entropy)
â”‚   â”œâ”€â”€ tokenizer.py           # CognitiveTokenizer
â”‚   â””â”€â”€ datasets.py            # Text/Image/Video datasets
â”œâ”€â”€ kernels/
â”‚   â”œâ”€â”€ hamiltonian.py         # Hamiltonian operator H[T]
â”‚   â”œâ”€â”€ bayesian.py            # Bayesian update Î›_QR[T]
â”‚   â””â”€â”€ fractional_memory.py   # Fractional memory Î›_F[T]
â”œâ”€â”€ operators/
â”‚   â””â”€â”€ collapse.py            # Field collapse and measurement
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics.py             # Training metrics and diagnostics
â”‚   â””â”€â”€ visualization.py       # Plotting utilities
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ examples/                  # Usage examples
â”‚   â”œâ”€â”€ geometric_inference.py # Field-based inference demo
â”‚   â””â”€â”€ mnist_clustering.py    # Self-tokenization (WIP)
â””â”€â”€ main.py                    # Interactive training interface
```

## Key Parameters

| Symbol | Name | Default | Range | Description |
|--------|------|---------|-------|-------------|
| â„_cog | Cognitive Planck constant | 0.1 | 0.01-1.0 | Sets quantum-like evolution scale |
| m_cog | Effective mass | 1.0 | 0.1-10.0 | Controls diffusion rate |
| Î»_QR | Bayesian update gain | 0.3 | 0.1-0.5 | Belief revision strength |
| Î»_F | Memory damping | 0.05 | 0.01-0.1 | Fractional memory strength |
| Î± | Fractional order | 0.5 | 0.3-0.7 | Memory decay rate (power-law exponent) |
| Ï„ | Bayesian temperature | 0.5 | 0.1-1.0 | Evidence sharpness |
| Î½ | Geodesic coupling | 1.0 | 0.1-10.0 | Field-embedding coupling strength |
| D | Tensor dimension | 16 | â‰¥16 | Internal DOF (must be â‰¥16 for overdetermination) |

### Adaptive Learning

When `adaptive_learning=True`, parameters Î±, Î½, and Ï„ become learnable spatial fields that optimize via entropy gradients:

```python
âˆ‚Î±/âˆ‚t = -Î·_Î± âˆ‚H/âˆ‚Î±    # Minimize field entropy
âˆ‚Î½/âˆ‚t = -Î·_Î½ âˆ‚H/âˆ‚Î½    # Optimize coupling
âˆ‚Ï„/âˆ‚t = -Î·_Ï„ âˆ‚H/âˆ‚Ï„    # Adjust temperature
```

where H = -Tr(T log T) is the field entropy.

## Features & Capabilities

### Implemented âœ“

**Core Field Dynamics:**
- Complete tensor field evolution (master equation)
- All three kernel operators (H, Î›_QR, Î›_F)
- O(1) LIoR memory kernel with multi-mode recurrence
- Adaptive parameter learning (Î±, Î½, Ï„)
- Biquaternion state representation (16-DOF)

**Geometric Attention:**
- Field-contracted geometric products (wedge/tensor/spinor)
- Memory-efficient attention (O(seqÂ²) instead of O(seqÂ²Â·dÂ²))
- Multiple attention modes with learned mixing weights

**Training Infrastructure:**
- LIoR geodesic optimization
- Entropy-based parameter updates
- Standard trainer with LM/contrastive/alignment losses
- Comprehensive metrics and logging
- Checkpoint management

**Data Support:**
- Text datasets with cognitive tokenization
- Image/video dataset interfaces
- Multi-modal data loading

### Current Limitations âš 

- MNIST self-tokenization example incomplete
- Visualization utilities basic
- DPR integration optional (requires transformers library)
- Some geometric inference examples need updating

### Research Directions ğŸ“‹

- **Semantic Addressing**: Metric tensor + Christoffel symbols for navigating concept space
- **Route Hashing**: BCH error correction for stable addressing
- **Neighbor Structures**: Efficient k-NN in field space
- **Active Inference**: Integrate free energy principle
- **Multi-scale Fields**: Hierarchical field resolutions

## Testing

Run the test suite:

```bash
pytest tests/ -v
```

Test specific components:
```bash
pytest tests/test_conservation.py  # Norm conservation
pytest tests/test_bayesian.py      # Bayesian updates
pytest tests/test_memory.py        # Fractional memory
```

## Examples

### Field Evolution Demo

```bash
python examples/geometric_inference.py
```

Shows how the field evolves and connects to transformer inference.

### MNIST Clustering (WIP)

```bash
python examples/mnist_clustering.py
```

Demonstrates emergent clustering via field dynamics (work in progress).

## Citation

If you use this code in your research, please cite:

```bibtex
@software{liorhybrid2025,
  title={Liorhybrid: Physics-Inspired AI with Bayesian Cognitive Fields},
  author={Leizerman, Sam},
  year={2025},
  url={https://github.com/JewJitsu11B/Liorhybrid}
}
```

## License

[To be determined]

## Contributing

Contributions are welcome! Please open an issue to discuss major changes.

## Contact

For questions or collaboration:
- Open an issue on [GitHub](https://github.com/JewJitsu11B/Liorhybrid/issues)
- See documentation files: `QUICK_START.md`, `IMPLEMENTATION_SUMMARY.md`, `TRAINING.md`

